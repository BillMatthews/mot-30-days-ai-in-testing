{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Walkthrough 1: Running Local Large Language Models (LLMs)\n",
    "\n",
    "A key risks raised repeatedly within the challenge was that of Data Privacy when using an LLM. When we interact with an LLM we sometimes need to provide additional context which may include data that your company does not want to be sent over the internet to a 3rd party. While companies such as OpenAI (ChatGPT) and Microsoft (Bing Copilot) allow you to opt out from your data being used in training many companies prefer to remove the risk by using Open-Source LLMs locally.\n",
    "\n",
    "In recent years there has been a growth in Open Source LLMs (such as Llama 2 from Meta or BERT from Google) - they use similar architectures to ChatGPT and Bing Copilot and share similar approaches to training the models.  These allow companies, if they want, to run LLMs locally.\n",
    "\n",
    "Running an LLM locally does have some drawbacks:\n",
    "* Generally larger LLM models will perform better but will require greater compute (memory and CPU/GPU) to be performant.\n",
    "* The company needs to provide the infrastructure to host and run the model\n",
    "* As the models evolve, the companies need to manage the model upgrades\n",
    "\n",
    "However, running local LLMS models have a number of key benefits:\n",
    "* Enhanced Data Security and Privacy since no data is sent to 3rd parties\n",
    "* Cost saving and reduction in vendor lock in\n",
    "* Ability to customise the LLM for their purposes\n",
    "\n",
    "This walkthrough will show you 2 ways to do this:\n",
    "* Using LlamaFile\n",
    "* Using HuggingFace\n",
    "\n",
    "> NOTE: This is not an exhaustive guide to deploying LLMs locally, instead it is to show you what is possible \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "876b7e6d60c18e1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Related Resources:\n",
    "\n",
    "| Link                                               | Description                                                                                                                                             |\n",
    "|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| https://www.datacamp.com/blog/top-open-source-llms | Introductory article on using Open Source LLMs                                                                                                          |\n",
    "| https://huggingface.co/models| Link to the HuggingFace Pre-trained Models Hub|\n",
    "| https://huggingface.co/learn/nlp-course            | A free course provided by HuggingFace that gets you up to speed with Natural Language Processing using the HuggingFace library (requires Python knowledge |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0a317b03c1a965"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using LlamaFile\n",
    "If you like the conversational style of tools such as Bing Copilot or ChatGPT you can host your own LLM in your desktop or a local server using an interesting project from Mozilla (https://www.mozilla.org/) called **LlamaFile**.\n",
    "\n",
    "The **LlmaFile** project aims to package up Open-Source Large Language Models into an executable that can be run as local webserver with a simple Chat Inferface and an API for you to query.\n",
    "\n",
    "The project can be found at https://github.com/Mozilla-Ocho/llamafile\n",
    "\n",
    "The project's ReadMe file contains instructions on how you can download an image and get it to run on your local machine.\n",
    "\n",
    "There are a few points to remember:\n",
    "* The LLM models you can access are limited to Open-Source models - you won't find models such as GTP3.5, GPT4 available in LlamaFile as these are closed source.\n",
    "* Large Language Models take a large amount of memory, so it's unlikely that you will be able to run a model the size of GPT on your desktop and so performance may not be as good. \n",
    "* The text generation may be slow depending on the power of your local machine.\n",
    "\n",
    "> IMPORTANT: You will need to be able to run arbitrary executables on your machine. \n",
    "> Some company IT Security may prohibit this so please check before downloading and attempting to run the LlamaFile on company machines. \n",
    "\n",
    "For today's task I would suggest:\n",
    "1. Pick one of the smaller models liked on LlamaFile.\n",
    "2. Download the file and follow the instructions to run the LlamaFile\n",
    "3. Explore some conversations with your LLM.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "251905c45ada98b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using LLMs programmatically using HuggingFace Models\n",
    "\n",
    "During the challenge we used LLMs by typing in our prompts and we enhanced our prompts through various Prompt Engineering methods. Writing prompts into a chat style interface is certainly one way of interacting with a LLM, however it is not the only way we could use LLMs.\n",
    "\n",
    "Imagine you've crafted a prompt to assess the quality of a defect report, perhaps it scores the defect report in terms of clarity and completeness and if the defect is lacking the LLM outputs a set of questions for the defect author. \n",
    "\n",
    "This might be a useful addition to your defect handling workflow but let's face it, if you needed to type (or copy) the prompt and the defect description into a Chat Interface for each defect it will quickly become tedious.\n",
    "\n",
    "Instead, we can programmatically extract any new defects and for each one call an LLM to evaluate the defect report. We can achieve this in a straightforward manner using LLM from HuggingFace and a bit of Python code.\n",
    "\n",
    "> For this walkthrough, you do not need to write any code. The code presented here is complete and designed to show we could integrate AI models. If you want to learn more about the coding aspect of integrating AI models then I would suggest you start with the HuggingFace NLP Course.\n",
    "\n",
    "HuggingFace (https://huggingface.co) is a machine learning community that collaborates on building models and developing datasets. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d74f8ea242adee81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to install some dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75345ac864b1187c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers bitsandbytes>=0.39.0 accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7bfeb8ea99e995f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will use the Huggingface Transformer Library to create a pre-trained model based on the Open-Source Mistral-7B model.\n",
    "\n",
    "When using HuggingFace, this is all the code that is needed!\n",
    "This will download the model (this may take some time to complete) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d0b143306dddb98"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoModelForCausalLM\n\u001B[0;32m      3\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmistralai/Mistral-7B-v0.1\u001B[39m\u001B[38;5;124m\"\u001B[39m \n\u001B[1;32m----> 5\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mload_in_8bit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mot-30-days-ai-in-testing-ZP-QyxVu-py3.10\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    561\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    562\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    564\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    565\u001B[0m     )\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    567\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    569\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\mot-30-days-ai-in-testing-ZP-QyxVu-py3.10\\lib\\site-packages\\transformers\\modeling_utils.py:2970\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   2966\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2967\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2968\u001B[0m         )\n\u001B[0;32m   2969\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[1;32m-> 2970\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m   2971\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2972\u001B[0m         )\n\u001B[0;32m   2974\u001B[0m \u001B[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001B[39;00m\n\u001B[0;32m   2975\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_in_4bit \u001B[38;5;129;01mor\u001B[39;00m load_in_8bit:\n",
      "\u001B[1;31mImportError\u001B[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:31:06.736892Z",
     "start_time": "2024-03-25T16:31:06.520169Z"
    }
   },
   "id": "148fd0762f66f89c",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you remember when you created your prompts earlier in the challenge, you used fairly natural langauge to specify what you wanted.\n",
    "\n",
    "However, machines don't really understand text, they only deal with numbers so we need to perform a task called Tokenization, which converts the text we type into a numerical form that the model can process. Moreover, the method we use to tokenize our text needs to be one that the model understands.\n",
    "\n",
    "Tokenization is an involved process but again, HuggingFace make this really easy for us and with just a few lines of code we can tokenize sentences.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2359948081859ec"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac58407f5d5f45bda5c3e97ad5072811"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a484804429f41298c0c98730add361a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f20f132e7d44750bdc6b65ad2a8344a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3848c1cdfe574089b2f4ba077ad0e14e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:28:43.438515Z",
     "start_time": "2024-03-25T16:28:41.096640Z"
    }
   },
   "id": "f3d61ec6573a6541",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand what the Tokenizer does, you can run the following cell with some text to see what the model receives as input"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dabbee9c67c0bc84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   315, 28742,   333,  5597,  7368,   272, 17036,   302,  3735,\n",
      "           288, 28705, 28770, 28734, 19503,   302, 16107,   297,  3735,   288,\n",
      "          8035]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "sample_input = \"I've nearly completed the Ministry of Testing 30 Days of AI in Testing challenge\"\n",
    "sample_model_input = model_inputs = tokenizer([sample_input], return_tensors=\"pt\")\n",
    "\n",
    "print(sample_model_input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:28:52.955589Z",
     "start_time": "2024-03-25T16:28:52.936267Z"
    }
   },
   "id": "6f30568cc461f7ba",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see from the above output, our text gets converted into a large set of numbers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336227cf766126cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To interact with the LLM model we need to provide our prompt and call the *generate()* method for the model.\n",
    "The model will output a *tokenized* version of the output that we then need to decode back into text. \n",
    "\n",
    "Yet again, HuggingFace makes this a straightforward task. For convince we've wrapped all the HuggingFace code for generation into the following function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "195bba17e2d16cf0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_model_response(model, tokenizer,  model_prompt) -> str:\n",
    "    \"\"\"\"\n",
    "    This function wraps the calls to the tokenizer to tokenize the model_prompt, calls the model's generate function then decodes the response.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer([sample_input], return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**tokens)\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:29:06.202956Z",
     "start_time": "2024-03-25T16:29:06.192933Z"
    }
   },
   "id": "a6caa24d760284d7",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want to generate some output. You can change the prompt text to be whatever you want. \n",
    "You can also change the text and re-run the cell as many times as you want."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d9d8276d0387eb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m prompt_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat are the key risks associated with using AI for decision making?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(get_model_response(\u001B[43mmodel\u001B[49m, tokenizer, prompt_text))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "prompt_text = \"What are the key risks associated with using AI for decision making?\"\n",
    "\n",
    "print(get_model_response(model, tokenizer, prompt_text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T16:29:10.393597Z",
     "start_time": "2024-03-25T16:29:10.371089Z"
    }
   },
   "id": "84db7b0750e8cfca",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that is it - using libraries such as HuggingFace can simplify the building of AI empowered tools. \n",
    "The simple and versatile programming library along with a large (and ever growing) pre-trained model hub is a powerful combination.\n",
    "\n",
    "If you want to learn more about building using HuggingFace, I would recommend the HuggingFace NLP Course (https://huggingface.co/learn/nlp-course)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fa5369da0c3acdf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6d86bf296f70c51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
