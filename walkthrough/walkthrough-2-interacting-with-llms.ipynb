{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Walkthrough 2: Interacting with LLMs programmatically\n",
    "\n",
    "If you completed Walkthrough 1, you will know that a key risk of using public services such as ChatGPT is Data Privacy and we showed that we can run Open Source LLMs locally using initiatives such as Mozilla LlamaFile (https://github.com/Mozilla-Ocho/llamafile). However, this is not the only way we can access and use Open source LLMs and we will show you one way of doing this in this walkthrough. \n",
    "\n",
    "**Note** we can also access closed source LLMs such as GPT3.5/4.0 (which power ChatGPT) and while these can be private and generate good output, there is a cost associated with these.\n",
    "\n",
    "Additionally, while you may have been using platforms such as ChatGPT or Copilot in a conversational manner where you have a sequence of interactions with the model and the model remembers (up to a point) the previous interactions, this is not the only way to interact with LLMs.\n",
    "\n",
    "\n",
    "In this walkthrough we will use an Open Source LLM from HuggingFace and the HuggingFace Library to programmatically interact with a model.\n",
    "\n",
    ">HuggingFace (https://huggingface.co) is an amazing platform and community that focuses on building models and datasets for Machine Learning. They also provide a Python Library that simplifies many of the tasks we need to perform to load and interact with models.\n",
    "\n",
    "HuggingFace provides a set of pre-trained models that we can use locally if we want to protect our data or hosted. For today's walkthrough we will use a model locally since we are mostly addressing Data Privacy. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b57f8204fde7491c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, another interesting aspect of being able to programmatically interact with our models is that we can start to build more interesting AI based solutions by chaining different models together, enable our solutions to access other sources of data and even other tools, rather than trying to express everything as a Prompt.\n",
    "\n",
    "HuggingFace is a very popular library to build such applications so if you want to dive more deeply into this I would recommend the free HuggingFace NLP Course (https://huggingface.co/learn/nlp-course).\n",
    "\n",
    "I would also recommend investigating the LangChain library (https://www.langchain.com/langchain) for building AI Agents and complex AI based applications."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3267daa790fa1575"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Related Resources:\n",
    "\n",
    "| Link                                                       | Description                                                                                                                                                |\n",
    "|------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| https://www.datacamp.com/blog/top-open-source-llms         | Introductory article on using Open Source LLMs                                                                                                             |\n",
    "| https://huggingface.co/models                              | Link to the HuggingFace Pre-trained Models Hub                                                                                                             |\n",
    "| https://huggingface.co/learn/nlp-course                    | A free course provided by HuggingFace that gets you up to speed with Natural Language Processing using the HuggingFace library (requires Python knowledge) |\n",
    "| https://python.langchain.com/docs/get_started/introduction | Free tutorial provided by LangChain (requires Python knowledge)                                                                                            |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9949519342e5bd6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using LLMs programmatically using HuggingFace Models\n",
    "\n",
    "During the challenge we used LLMs by typing in our prompts and we enhanced our prompts through various Prompt Engineering methods. Writing prompts into a chat style interface is certainly one way of interacting with a LLM, however it is not the only way we could use LLMs.\n",
    "\n",
    "Imagine you've crafted a prompt to assess the quality of a defect report, perhaps it scores the defect report in terms of clarity and completeness and if the defect is lacking the LLM outputs a set of questions for the defect author. \n",
    "\n",
    "This might be a useful addition to your defect handling workflow but let's face it, if you needed to type (or copy) the prompt and the defect description into a Chat Interface for each defect it will quickly become tedious.\n",
    "\n",
    "Instead, we can programmatically extract any new defects and for each one call an LLM to evaluate the defect report. We can achieve this in a straightforward manner using LLM from HuggingFace and a bit of Python code.\n",
    "\n",
    "> For this walkthrough, you do not need to write any code. The code presented here is complete and designed to show we could integrate AI models. If you want to learn more about the coding aspect of integrating AI models then I would suggest you start with the HuggingFace NLP Course.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372595fba88b1c98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's Get Started\n",
    "\n",
    "This walkthrough has the following elements\n",
    "\n",
    "1. Install some required Dependencies\n",
    "2. Download and configure the Open Source LLM model\n",
    "3. Start interacting with the model\n",
    "\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "The LLM model we are running needs a GPU so, before we begin running code cells we need to switch to using a GPU.\n",
    "\n",
    "> You can do this by selecting \"Runtime -> Change Runtime Type\" from the Colab menu.\n",
    ">\n",
    "> Then select the \"T4 GPU\" option and click on \"Save\"\n",
    "\n",
    "This will give you access to run on a small GPU processor for a period of time.\n",
    "Do this now before continuing with the walkthrough.\n",
    "\n",
    "Notes:\n",
    "* Occasionally Colab will not have any available GPUs for you to use; if this happens you will need to try again later.\n",
    "* The name of the Runtime may be different in different regions so just pick one that has \"GPU\" in the label."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e97efcd6e4210288"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Install Dependencies First we need to install some dependencies\n",
    "Run the following code cells to install some dependencies into your Colab environment.\n",
    "\n",
    "This can take a few minutes to complete."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89230d948aa8ee40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers bitsandbytes>=0.39.0 accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Download and configure the Open Source LLM \n",
    "\n",
    "Next we will use the Huggingface Transformer Library to create a pre-trained model based on the `Open-Source Mistral-7B` model.\n",
    "\n",
    "This model has 7 Billion parameters which may sound large but if you consider that GPT3.5 (which powers ChatGPT) has approximately 20 Billion parameters, it is quite small really. However, it is about the largest model we can use within Colab. \n",
    "\n",
    "The HuggingFace Transformer Library makes loading a model very easy...Just one line of code \n",
    "\n",
    "Running the following cell will download the model - this could take a few minutes to complete"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc550ebfbb01fe75"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e91cb9ec955f3359"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f27ecfaabc6af8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you remember when you created your prompts earlier in the challenge, you used fairly natural langauge to specify what you wanted.\n",
    "\n",
    "However, machines don't really understand text, they only deal with numbers so we need to perform a task called Tokenization, which converts the text we type into a numerical form that the model can process. Moreover, the method we use to tokenize our text needs to be one that the model understands.\n",
    "\n",
    "Tokenization is an involved process but again, HuggingFace make this really easy for us and with just a few lines of code we can tokenize sentences.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f79eb98fb5a33f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8585dfa1f294c45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand what the Tokenizer does, you can run the following cell with some text to see what the model receives as input"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8cc7a6ed81817c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_input = \"I've nearly completed the Ministry of Testing 30 Days of AI in Testing challenge\"\n",
    "sample_model_input = model_inputs = tokenizer([sample_input], return_tensors=\"pt\")\n",
    "\n",
    "print(sample_model_input)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5d50f3e630054ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see from the above output, our text gets converted into a large set of numbers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c10f508933f6ffc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Start interacting with the model\n",
    "To interact with the LLM model we need to provide our prompt and call the *generate()* method for the model.\n",
    "The model will output a *tokenized* version of the output that we then need to decode back into text. \n",
    "\n",
    "Yet again, HuggingFace makes this a straightforward task. For convince we've wrapped all the HuggingFace code for generation into the following function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "343cbf1bd32ab53"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_model_response(model, tokenizer,  model_prompt) -> str:\n",
    "    \"\"\"\"\n",
    "    This function wraps the calls to the tokenizer to tokenize the model_prompt, calls the model's generate function then decodes the response.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer([sample_input], return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**tokens)\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c83b40d472c8622"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's create a prompt and get a response from our local model.\n",
    "\n",
    "The following cell contains a `prompt_text` and you can replace the text between the quotation marks with your own prompt.\n",
    "\n",
    "You can also change the text and re-run the cell as many times as you want."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ba3f94297fa44a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_text = \"What are the key risks associated with using AI for decision making?\"\n",
    "\n",
    "print(get_model_response(model, tokenizer, prompt_text))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d5ca3d2fff3ec1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that is it - using libraries such as HuggingFace can simplify the building of AI empowered tools. \n",
    "The simple and versatile programming library along with a large (and ever-growing) pre-trained model hub is a powerful combination.\n",
    "\n",
    "If you want to learn more about building using HuggingFace, I would recommend the HuggingFace NLP Course (https://huggingface.co/learn/nlp-course)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d36651f9ee6d82f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Questions for Reflection\n",
    "This walkthrough covered a basic setup for programmatically interacting with a local LLM, when working with models in this way, we have a lot of scope for adding bespoke logic, chaining interactions and integrating external tool use into our AI empowered tooling. From an AI in Testing perspective, this can open up a world beyond prompt engineering and allow teams to innovate on how they use AI in Testing. \n",
    "\n",
    "Before closing this workbook, reflect on the following questions:\n",
    "\n",
    "1. How might type of approach be used to address other concerns raised about using AI in Testing?\n",
    "2. How might your team use this type of approach to build AI empowered applications or integrate LLMs into existing test tooling you use?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5d460a19b73eb87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
