{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b57f8204fde7491c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using LLMs programmatically using HuggingFace Models\n",
    "\n",
    "During the challenge we used LLMs by typing in our prompts and we enhanced our prompts through various Prompt Engineering methods. Writing prompts into a chat style interface is certainly one way of interacting with a LLM, however it is not the only way we could use LLMs.\n",
    "\n",
    "Imagine you've crafted a prompt to assess the quality of a defect report, perhaps it scores the defect report in terms of clarity and completeness and if the defect is lacking the LLM outputs a set of questions for the defect author. \n",
    "\n",
    "This might be a useful addition to your defect handling workflow but let's face it, if you needed to type (or copy) the prompt and the defect description into a Chat Interface for each defect it will quickly become tedious.\n",
    "\n",
    "Instead, we can programmatically extract any new defects and for each one call an LLM to evaluate the defect report. We can achieve this in a straightforward manner using LLM from HuggingFace and a bit of Python code.\n",
    "\n",
    "> For this walkthrough, you do not need to write any code. The code presented here is complete and designed to show we could integrate AI models. If you want to learn more about the coding aspect of integrating AI models then I would suggest you start with the HuggingFace NLP Course.\n",
    "\n",
    "HuggingFace (https://huggingface.co) is a machine learning community that collaborates on building models and developing datasets. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "372595fba88b1c98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we need to install some dependencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89230d948aa8ee40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers bitsandbytes>=0.39.0 accelerate -q"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we will use the Huggingface Transformer Library to create a pre-trained model based on the Open-Source Mistral-7B model.\n",
    "\n",
    "When using HuggingFace, this is all the code that is needed!\n",
    "This will download the model (this may take some time to complete) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e91cb9ec955f3359"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f27ecfaabc6af8a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you remember when you created your prompts earlier in the challenge, you used fairly natural langauge to specify what you wanted.\n",
    "\n",
    "However, machines don't really understand text, they only deal with numbers so we need to perform a task called Tokenization, which converts the text we type into a numerical form that the model can process. Moreover, the method we use to tokenize our text needs to be one that the model understands.\n",
    "\n",
    "Tokenization is an involved process but again, HuggingFace make this really easy for us and with just a few lines of code we can tokenize sentences.   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9f79eb98fb5a33f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8585dfa1f294c45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To understand what the Tokenizer does, you can run the following cell with some text to see what the model receives as input"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8cc7a6ed81817c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_input = \"I've nearly completed the Ministry of Testing 30 Days of AI in Testing challenge\"\n",
    "sample_model_input = model_inputs = tokenizer([sample_input], return_tensors=\"pt\")\n",
    "\n",
    "print(sample_model_input)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5d50f3e630054ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see from the above output, our text gets converted into a large set of numbers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c10f508933f6ffc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To interact with the LLM model we need to provide our prompt and call the *generate()* method for the model.\n",
    "The model will output a *tokenized* version of the output that we then need to decode back into text. \n",
    "\n",
    "Yet again, HuggingFace makes this a straightforward task. For convince we've wrapped all the HuggingFace code for generation into the following function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "343cbf1bd32ab53"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_model_response(model, tokenizer,  model_prompt) -> str:\n",
    "    \"\"\"\"\n",
    "    This function wraps the calls to the tokenizer to tokenize the model_prompt, calls the model's generate function then decodes the response.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer([sample_input], return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**tokens)\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c83b40d472c8622"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want to generate some output. You can change the prompt text to be whatever you want. \n",
    "You can also change the text and re-run the cell as many times as you want."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ba3f94297fa44a2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt_text = \"What are the key risks associated with using AI for decision making?\"\n",
    "\n",
    "print(get_model_response(model, tokenizer, prompt_text))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d5ca3d2fff3ec1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And that is it - using libraries such as HuggingFace can simplify the building of AI empowered tools. \n",
    "The simple and versatile programming library along with a large (and ever growing) pre-trained model hub is a powerful combination.\n",
    "\n",
    "If you want to learn more about building using HuggingFace, I would recommend the HuggingFace NLP Course (https://huggingface.co/learn/nlp-course)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d36651f9ee6d82f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
